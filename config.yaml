# LLM Configuration
llm:
  provider: openai
  model: gpt-4o
  temperature: 0.2
  api_key: ${OPENAI_API_KEY}  # Use environment variable

# Embeddings Configuration
embeddings:
  provider: openai
  model: text-embedding-3-small
  api_key: ${OPENAI_API_KEY}

# Processing Configuration
processing:
  batch_size: 5  # Reduced batch size for better memory management
  max_retries: 5  # Increased retries
  retry_delay: 2  # Increased initial retry delay
  max_concurrency: 3  # Reduced concurrency to avoid rate limits
  adaptive_rate_limiting: true  # Enable adaptive rate limiting

# Caching Configuration
caching:
  embeddings_cache_dir: ./cache/embeddings
  llm_cache_path: .langchain.db
  batch_results_dir: ./cache/batch_results  # Directory for temporary batch results

# Output Configuration
output:
  graph_format: png
  export_format: json
  visualization:
    figure_size: [12, 8]
    node_size: 700
    node_color: lightblue
    edge_color: gray
    font_size: 10

# Taxonomy Configuration
taxonomy:
  min_confidence: 0.6
  max_tags: 100
  chunk_size: 1000
  chunk_overlap: 200 